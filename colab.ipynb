{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On1_cnLYAs46"
      },
      "source": [
        "## Welcome to the (Un)Official KoboldCpp Colab Notebook\n",
        "It's really easy to get started. Just press the two **Play** buttons below, and then connect to the **Cloudflare URL** shown at the end.\n",
        "\n",
        "You can select a model from the dropdown or enter a HF model name or URL into the Model feild, formatting should look like `KoboldAI/LLaMA2-13B-Tiefighter-GGUF`, or `https://huggingface.co/TheBloke/MLewd-ReMM-L2-Chat-20B-Inverted-GGUF`.\n",
        "\n",
        "If a custom HF model name fails to load you may want to dircetly enter the link of the desired model. Formating for a manual URL should look like `https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q4_K_M.gguf` and is obtained by copying the link adress from the download arrow of the desired quant, doing this will use the quant value from the link instead of from the quant dropdown.\n",
        "\n",
        "For **13B** models The following configurations are tested and known to work\n",
        "- `Q4_K_M / 4096 context` Standard configuration\n",
        "- `Q4_K_M / 6144 context` Increased context with a small cost to quality and speed\n",
        "- `Q5_K_M / 4096 context` Increased quality at a small cost to speed\n",
        "\n",
        "For **7B** models The following configurations are tested and known to work\n",
        "- `Q5_K_M / 4096 context` Standard configuration\n",
        "- `Q5_K_M / 8192 context` Greatly increased context with a small cost to quality and speed"\n",
        "\n",
        "For **16B** experimental models The following configurations are tested and known to work\n",
        "- `Q4_K_S / 8192 context` Standard configuration\n",
        "- `Q4_K_S / 12288 context` Should also work\n",
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga0gRRsAAs47"
      },
      "outputs": [],
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <b>v-- Enter your model below and then click this to start Koboldcpp</b>\n",
        "\n",
        "Model = \"KoboldAI/LLaMA2-13B-Tiefighter-GGUF\" #@param [\"KoboldAI/LLaMA2-13B-Tiefighter-GGUF\",\"TheBloke/ReMM-SLERP-L2-13B-GGUF\",\"TheBloke/Noromaid-13B-v0.1.1-GGUF\",\"TheBloke/MythoMax-L2-13B-GGUF\",\"TheBloke/Xwin-LM-13B-v0.2-GGUF\",\"TheBloke/Stheno-L2-13B-GGUF\",\"KoboldAI/LLaMA2-13B-Psyfighter2-GGUF\",\"TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\",\"TheBloke/Mistral-7B-Claude-Chat-GGUF\",\"https://mergebox.koboldai.net/ColdMeds-16B-henkified-c1.gguf\"]{allow-input: true}\n",
        "Quant = \"Q4_K_M\" #@param [\"Q4_K_S\",\"Q4_K_M\",\"Q5_K_M\"] {allow-input: true}\n",
        "ContextSize = \"4096\" #@param [2048,4096,6144,8192] {allow-input: true}\n",
        "\n",
        "LocalTunnelBackup = False #@param {type:\"boolean\"}\n",
        "ForceRebuild = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "# Split the Model string into MODELPATH and MODELNAME\n",
        "# Check if the Model string contains \"huggingface.co\"\n",
        "if \"huggingface.co\" in Model:\n",
        "    # Extract the path after \"huggingface.co/\"\n",
        "    parts = Model.split(\"huggingface.co/\")[1].split('/')\n",
        "else:\n",
        "    # Assume it's the short version and split the Model string\n",
        "    parts = Model.split('/')\n",
        "MODELPATH = parts[0]\n",
        "MODELNAME = parts[1].replace('-GGUF', '')\n",
        "\n",
        "print(\"MODELPATH:\", MODELPATH)\n",
        "print(\"MODELNAME:\", MODELNAME)\n",
        "\n",
        "if ForceRebuild:\n",
        "  FLAG2 = \"F\"\n",
        "else:\n",
        "  FLAG2 = \"concedo\"\n",
        "\n",
        "\n",
        "\n",
        "if \"gguf\" not in Model.lower() and \"-ggml\" not in Model.lower():\n",
        "    raise ValueError(\"Error: The Model field does not contain the required string '-GGUF'. Please ensure that your selected model is in GGUF format and try again. Consider searching Huggingface with MODELNAME GGUF to find the desired model\")\n",
        "else:\n",
        "    print(\"Link check valid.\")\n",
        "\n",
        "if (\"ggml\" in Model and \".bin\" in Model.lower()):\n",
        "    print(\"You appear to be loading a GGML file, GGUF has replaced GGML. This notebook will work, but some options, such as automatic scaling for extended context, will not work\")\n",
        "    !sleep 10\n",
        "MODELURL = \"https://huggingface.co/{MODELPATH}/{MODELNAME}-GGUF/resolve/main/{MODELNAME}.{Quant}.gguf\"\n",
        "\n",
        "Model = Model.replace(\"blob\", \"resolve\")\n",
        "\n",
        "if \".gguf\" in Model:\n",
        "  MODELURL = Model\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/LostRuins/koboldcpp\n",
        "%cd /content/koboldcpp\n",
        "kvers = !(cat koboldcpp.py | grep 'KcppVersion = ' | cut -d '\"' -f2)\n",
        "kvers = kvers[0]\n",
        "!echo Finding prebuilt binary for {kvers}\n",
        "!wget -O dlfile.tmp https://kcppcolab.{FLAG2}.workers.dev/?{kvers} && mv dlfile.tmp koboldcpp_cublas.so\n",
        "!test -f koboldcpp_cublas.so && echo Prebuilt Binary Exists || echo Prebuilt Binary Does Not Exist\n",
        "!test -f koboldcpp_cublas.so && echo Build Skipped || make koboldcpp_cublas LLAMA_CUBLAS=1 LLAMA_COLAB=1\n",
        "!cp koboldcpp_cublas.so koboldcpp_cublas.dat\n",
        "!apt install aria2 -y\n",
        "!rm model.ggml\n",
        "if \".gguf\" in Model or \".bin\" in Model:\n",
        "  !aria2c -x 10 -o model.ggml --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Model\n",
        "else:\n",
        "  !aria2c -x 10 -o model.ggml --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none https://huggingface.co/{MODELPATH}/{MODELNAME}-GGUF/resolve/main/{MODELNAME}.{Quant}.gguf\n",
        "\n",
        "MODELNAME = MODELNAME.lower()\n",
        "import os\n",
        "\n",
        "\n",
        "folder_path = '/content/koboldcpp'\n",
        "file_name = 'model.ggml'\n",
        "\n",
        "full_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "if os.path.exists(full_path):\n",
        "    print(f\"The file {file_name} is present in the folder. Not redownloading\")\n",
        "else:\n",
        "    print(f\"The file {file_name} is not present in the folder.\")\n",
        "    !aria2c -x 10 -o model.ggml --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none  https://huggingface.co/{MODELPATH}/{MODELNAME}-GGUF/resolve/main/{MODELNAME}.{Quant}.gguf\n",
        "\n",
        "if os.path.exists(full_path):\n",
        "    print(f\"The file {file_name} is present in the folder. Continuing\")\n",
        "else:\n",
        "    print(f\"The file {file_name} is not present in the folder.\")\n",
        "    print(f\"The file is not present in the folder. All attempts to download the file based on entered Model name have failed. Please use the direct link method instead\")\n",
        "    !sleep 120\n",
        "if LocalTunnelBackup:\n",
        "  !npm install -g localtunnel\n",
        "  !nohup lt --port 5001 &\n",
        "  !sleep 8\n",
        "  !cat nohup.out\n",
        "\n",
        "\n",
        "if LocalTunnelBackup:\n",
        "  !python koboldcpp.py model.ggml --usecublas 0 --multiuser --gpulayers 99 --contextsize $ContextSize --hordeconfig $MODELNAME 1 1 --onready \"curl -s ipecho.net/plain | xargs -I % echo If using the Localtunnel backup scroll up for your Localtunnel link, then use the following IP to verify: %\"\n",
        "else:\n",
        "  !python koboldcpp.py model.ggml --usecublas 0 --multiuser --gpulayers 99 --contextsize $ContextSize --hordeconfig $MODELNAME 1 1 --remotetunnel"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
