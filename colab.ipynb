{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On1_cnLYAs46"
      },
      "source": [
        "## Welcome to the (Un)Official KoboldCpp Colab Notebook\n",
        "It's really easy to get started. Just press the two **Play** buttons below, and then connect to the **Cloudflare URL** shown at the end.\n",
        "\n",
        "You can select a model from the dropdown or enter a HF model name or URL into the Model feild, formatting should look like `KoboldAI/LLaMA2-13B-Tiefighter-GGUF`, or `https://huggingface.co/TheBloke/MLewd-ReMM-L2-Chat-20B-Inverted-GGUF`.\n",
        "\n",
        "If a custom HF model name fails to load you may want to dircetly enter the link of the desired model. Formating for a manual URL should look like `https://huggingface.co/KoboldAI/LLaMA2-13B-Tiefighter-GGUF/resolve/main/LLaMA2-13B-Tiefighter.Q4_K_M.gguf` and is obtained by copying the link adress from the download arrow of the desired quant, doing this will use the quant value from the link instead of from the quant dropdown.\n",
        "\n",
        "For **13B** models The following configurations are tested and known to work\n",
        "- `Q4_K_M / 4096 context` Standard configuration\n",
        "- `Q4_K_M / 6144 context` Increased context with a small cost to quality and speed\n",
        "- `Q5_K_M / 4096 context` Increased quality at a small cost to speed\n",
        "\n",
        "For **7B** models The following configurations are tested and known to work\n",
        "- `Q5_K_M / 4096 context` Standard configuration\n",
        "- `Q5_K_M / 8192 context` Greatly increased context with a small cost to quality and speed\n",
        "- `Q5_K_M / 16384 context` Will load, may suffer quality loss\n",
        "\n",
        "For **11B** models The following configurations are tested and known to work.\n",
        "- `Q5_K_M / 8192 context` Standard configuration\n",
        "- `Q5_K_M / 12288 context` This should also work\n",
        "- `Q5_K_M / 16384 context` Will load, may suffer quality loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ga0gRRsAAs47"
      },
      "outputs": [],
      "source": [
        "#@title <-- Tap this if you play on Mobile { display-mode: \"form\" }\n",
        "%%html\n",
        "<b>Press play on the music player to keep the tab alive, then start KoboldCpp below</b><br/>\n",
        "<audio src=\"https://raw.githubusercontent.com/KoboldAI/KoboldAI-Client/main/colab/silence.m4a\" controls>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJS9i_Dltv8Y",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title <b>v-- Enter your model below and then click this to start Koboldcpp</b>\n",
        "\n",
        "Model = \"KoboldAI/LLaMA2-13B-Tiefighter-GGUF\" #@param [\"KoboldAI/LLaMA2-13B-Tiefighter-GGUF\",\"TheBloke/ReMM-SLERP-L2-13B-GGUF\",\"TheBloke/Noromaid-13B-v0.1.1-GGUF\",\"TheBloke/MythoMax-L2-13B-GGUF\",\"TheBloke/Xwin-LM-13B-v0.2-GGUF\",\"TheBloke/Stheno-L2-13B-GGUF\",\"KoboldAI/LLaMA2-13B-Psyfighter2-GGUF\",\"TheBloke/OpenHermes-2.5-Mistral-7B-GGUF\",\"https://huggingface.co/Sao10K/Fimbulvetr-11B-v2-GGUF/resolve/main/Fimbulvetr-11B-v2-Test-14.q5_K_M.gguf\"]{allow-input: true}\n",
        "Quant = \"Q4_K_M\" #@param [\"Q4_K_S\",\"Q4_K_M\",\"Q5_K_M\"] {allow-input: true}\n",
        "ContextSize = \"4096\" #@param [2048,4096,6144,8192,12288] {allow-input: true}\n",
        "\n",
        "LocalTunnelBackup = False #@param {type:\"boolean\"}\n",
        "ForceRebuild = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <hr>\n",
        "LoadLLaVAmmproj = False #@param {type:\"boolean\"}\n",
        "LLaVAmmproj = \"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\" #@param [\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-13b-mmproj-v1.5.Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/mistral-7b-mmproj-v1.5-Q4_1.gguf\",\"https://huggingface.co/koboldcpp/mmproj/resolve/main/llama-7b-mmproj-v1.5-Q4_0.gguf\"]{allow-input: true}\n",
        "VCommand = \"\"\n",
        "#@markdown <hr>\n",
        "LoadImgModel = False #@param {type:\"boolean\"}\n",
        "ImgModel = \"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_older_q4_0.gguf\" #@param [\"https://huggingface.co/koboldcpp/imgmodel/resolve/main/imgmodel_older_q4_0.gguf\"]{allow-input: true}\n",
        "SCommand = \"\"\n",
        "\n",
        "import os\n",
        "if not os.path.isfile(\"/opt/bin/nvidia-smi\"):\n",
        "  raise RuntimeError(\"⚠️Colab did not give you a GPU due to usage limits, this can take a few hours before they let you back in. Check out https://lite.koboldai.net for a free alternative (that does not provide an API link but can load KoboldAI saves and chat cards) or subscribe to Colab Pro for immediate access.⚠️\")\n",
        "if LLaVAmmproj and LoadLLaVAmmproj:\n",
        "  VCommand = \"--mmproj vmodel.gguf\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "if ImgModel and LoadImgModel:\n",
        "  SCommand = \"--sdconfig imodel.gguf clamped 4 quant\"\n",
        "else:\n",
        "  SCommand = \"\"\n",
        "\n",
        "\n",
        "# Split the Model string into MODELPATH and MODELNAME\n",
        "# Check if the Model string contains \"huggingface.co\"\n",
        "if \"huggingface.co\" in Model:\n",
        "    # Extract the path after \"huggingface.co/\"\n",
        "    parts = Model.split(\"huggingface.co/\")[1].split('/')\n",
        "else:\n",
        "    # Assume it's the short version and split the Model string\n",
        "    parts = Model.split('/')\n",
        "MODELPATH = parts[0]\n",
        "MODELNAME = parts[1].replace('-GGUF', '')\n",
        "\n",
        "print(\"MODELPATH:\", MODELPATH)\n",
        "print(\"MODELNAME:\", MODELNAME)\n",
        "\n",
        "if ForceRebuild:\n",
        "  FLAG2 = \"F\"\n",
        "else:\n",
        "  FLAG2 = \"concedo\"\n",
        "\n",
        "\n",
        "\n",
        "if \"gguf\" not in Model.lower() and \"-ggml\" not in Model.lower():\n",
        "    raise ValueError(\"Error: The Model field does not contain the required string '-GGUF'. Please ensure that your selected model is in GGUF format and try again. Consider searching Huggingface with MODELNAME GGUF to find the desired model\")\n",
        "else:\n",
        "    print(\"Link check valid.\")\n",
        "\n",
        "if (\"ggml\" in Model and \".bin\" in Model.lower()):\n",
        "    print(\"You appear to be loading a GGML file, GGUF has replaced GGML. This notebook will work, but some options, such as automatic scaling for extended context, will not work\")\n",
        "    !sleep 10\n",
        "MODELURL = \"https://huggingface.co/{MODELPATH}/{MODELNAME}-GGUF/resolve/main/{MODELNAME}.{Quant}.gguf\"\n",
        "\n",
        "Model = Model.replace(\"blob\", \"resolve\")\n",
        "\n",
        "if \".gguf\" in Model:\n",
        "  MODELURL = Model\n",
        "\n",
        "\n",
        "%cd /content\n",
        "!git clone https://github.com/LostRuins/koboldcpp\n",
        "%cd /content/koboldcpp\n",
        "kvers = !(cat koboldcpp.py | grep 'KcppVersion = ' | cut -d '\"' -f2)\n",
        "kvers = kvers[0]\n",
        "!echo Finding prebuilt binary for {kvers}\n",
        "!wget -O dlfile.tmp https://kcppcolab.{FLAG2}.workers.dev/?{kvers} && mv dlfile.tmp koboldcpp_cublas.so\n",
        "!test -f koboldcpp_cublas.so && echo Prebuilt Binary Exists || echo Prebuilt Binary Does Not Exist\n",
        "!test -f koboldcpp_cublas.so && echo Build Skipped || make koboldcpp_cublas LLAMA_CUBLAS=1 LLAMA_COLAB=1\n",
        "!cp koboldcpp_cublas.so koboldcpp_cublas.dat\n",
        "!apt install aria2 -y\n",
        "!rm model.ggml\n",
        "if \".gguf\" in Model or \".bin\" in Model:\n",
        "  !aria2c -x 10 -o model.ggml --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $Model\n",
        "else:\n",
        "  !aria2c -x 10 -o model.ggml --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none https://huggingface.co/{MODELPATH}/{MODELNAME}-GGUF/resolve/main/{MODELNAME}.{Quant}.gguf\n",
        "\n",
        "if VCommand:\n",
        "  !aria2c -x 10 -o vmodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $LLaVAmmproj\n",
        "if SCommand:\n",
        "  !aria2c -x 10 -o imodel.gguf --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none $ImgModel\n",
        "\n",
        "\n",
        "MODELNAME = MODELNAME.lower()\n",
        "import os\n",
        "\n",
        "\n",
        "folder_path = '/content/koboldcpp'\n",
        "file_name = 'model.ggml'\n",
        "\n",
        "full_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "if os.path.exists(full_path):\n",
        "    print(f\"The file {file_name} is present in the folder. Not redownloading\")\n",
        "else:\n",
        "    print(f\"The file {file_name} is not present in the folder.\")\n",
        "    !aria2c -x 10 -o model.ggml --summary-interval=5 --download-result=default --allow-overwrite=true --file-allocation=none  https://huggingface.co/{MODELPATH}/{MODELNAME}-GGUF/resolve/main/{MODELNAME}.{Quant}.gguf\n",
        "\n",
        "if os.path.exists(full_path):\n",
        "    print(f\"The file {file_name} is present in the folder. Continuing\")\n",
        "else:\n",
        "    print(f\"The file {file_name} is not present in the folder.\")\n",
        "    print(f\"The file is not present in the folder. All attempts to download the file based on entered Model name have failed. Please use the direct link method instead\")\n",
        "    !sleep 120\n",
        "if LocalTunnelBackup:\n",
        "  !npm install -g localtunnel\n",
        "  !nohup lt --port 5001 &\n",
        "  !sleep 8\n",
        "  !cat nohup.out\n",
        "\n",
        "\n",
        "if LocalTunnelBackup:\n",
        "  !python koboldcpp.py model.ggml --usecublas 0 --multiuser --gpulayers 99 --contextsize $ContextSize $VCommand $SCommand --hordeconfig $MODELNAME 1 1 --onready \"curl -s ipecho.net/plain | xargs -I % echo If using the Localtunnel backup scroll up for your Localtunnel link, then use the following IP to verify: %\"\n",
        "else:\n",
        "  !python koboldcpp.py model.ggml --usecublas 0 --multiuser --gpulayers 99 --contextsize $ContextSize --hordeconfig $MODELNAME 1 1 --remotetunnel $VCommand $SCommand"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
